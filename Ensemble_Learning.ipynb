{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it."
      ],
      "metadata": {
        "id": "w2RDhy-OHrAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Ensemble Learning is a technique in machine learning where multiple models are combined to improve accuracy and robustness.\n",
        "\n",
        "Key idea: Different models make different errors, so combining them reduces mistakes.\n",
        "\n",
        "Main types:\n",
        "\n",
        "Bagging: Train models on random subsets → reduce variance (e.g., Random Forest)\n",
        "\n",
        "Boosting: Train models sequentially, focusing on mistakes → reduce bias (e.g., XGBoost)\n",
        "\n",
        "Stacking: Combine different models using a meta-model → leverage strengths of each\n",
        "\n",
        "Pros: More accurate, less overfitting.\n",
        "Cons: Slower, harder to interpret."
      ],
      "metadata": {
        "id": "ciqPi4NXHq9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "2CHVlpRTHq6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Bagging: Builds multiple models in parallel on random subsets and combines them to reduce variance (e.g., Random Forest).\n",
        "\n",
        "Boosting: Builds models sequentially, each focusing on previous errors to reduce bias (e.g., XGBoost).\n",
        "\n",
        "Key difference: Bagging focuses on stability, Boosting focuses on correcting mistakes.\n",
        "\n",
        "Bagging treats all models equally when combining predictions.\n",
        "\n",
        "Boosting weights models based on their performance, giving more importance to accurate ones."
      ],
      "metadata": {
        "id": "-KtiIMwHHq30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?"
      ],
      "metadata": {
        "id": "xiInQL75Hq1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Bootstrap Sampling:\n",
        "\n",
        "It is a method of creating random subsets of the training data by sampling with replacement.\n",
        "\n",
        "Some data points may appear multiple times in a subset, while others may be left out.\n",
        "\n",
        "Role in Bagging :\n",
        "\n",
        "Each decision tree in a Random Forest is trained on a different bootstrap sample, making the trees diverse.\n",
        "\n",
        "Each split is selected by randomly subset of features.\n",
        "\n",
        "Combining predictions from these diverse trees via voting or averaging reduces variance, reduce pverfitting and improves accuracy."
      ],
      "metadata": {
        "id": "SeZ0QMD6HqyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?"
      ],
      "metadata": {
        "id": "mvPDpC4IHqvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Out-of-Bag Samples:\n",
        "\n",
        "In bootstrap sampling, each model is trained on a random subset of data.\n",
        "\n",
        "The data points not included in a model’s training subset are called OOB samples.\n",
        "\n",
        "OOB Score:\n",
        "\n",
        "Used to evaluate ensemble models without a separate test set.\n",
        "\n",
        "Each OOB sample is predicted by the trees that did not see it during training, and these predictions are compared to the true labels.\n",
        "\n",
        "The average accuracy or error over all OOB samples gives the OOB score, a reliable estimate of model performance."
      ],
      "metadata": {
        "id": "J1bKvhvSHqsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n"
      ],
      "metadata": {
        "id": "m4nnYK9wHqqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Single Decision Tree:\n",
        "\n",
        "Feature importance is based on how much each feature reduces impurity (like Gini or entropy) in that one tree.\n",
        "\n",
        "Can be biased toward features with more levels or numerical range.\n",
        "\n",
        "Random Forest:\n",
        "\n",
        "Feature importance is averaged across all trees, making it more robust.\n",
        "\n",
        "Less biased, as multiple trees reduce the effect of any single tree’s errors."
      ],
      "metadata": {
        "id": "GPJw9HKmHqnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "Z_ZyHzyEHqku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArHgg0rsLBfd",
        "outputId": "be50869a-9d8b-452c-e315-779db761036b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "a1yFCG05HosD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "bagging = BaggingClassifier(\n",
        "    DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    bootstrap=True\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "accuracy_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "print(f\"Accuracy of Single Decision Tree: {accuracy_dt:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier: {accuracy_bag:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyRAYQw-LN65",
        "outputId": "cfb339b0-cae6-46f4-da6e-21ae3021415b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "8T5v2C-QMgTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Test Set Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HkzpJQuLkfQ",
        "outputId": "853eb7ca-cd37-4546-b367-5e617299eebc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Test Set Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context."
      ],
      "metadata": {
        "id": "Ms1JEYjyM5hl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Choosing Between Bagging or Boosting\n",
        "\n",
        "Bagging (Random Forest) → reduces variance, stable baseline.\n",
        "\n",
        "Boosting (XGBoost/LightGBM) → reduces bias, captures complex patterns.\n",
        "\n",
        "2. Handling Overfitting\n",
        "\n",
        "Limit tree depth, min samples per leaf.\n",
        "\n",
        "Use regularization and early stopping.\n",
        "\n",
        "Perform cross-validation to monitor performance.\n",
        "\n",
        "3. Selecting Base Models\n",
        "\n",
        "Use decision trees for flexibility and non-linear patterns.\n",
        "\n",
        "Optionally combine with logistic regression for stacking.\n",
        "\n",
        "4. Evaluating Performance\n",
        "\n",
        "Use k-fold cross-validation.\n",
        "\n",
        "Metrics: Accuracy, Precision, Recall, ROC-AUC.\n",
        "\n",
        "5. Justification for Ensemble Learning\n",
        "\n",
        "Combines multiple models → reduces errors.\n",
        "\n",
        "Provides more reliable loan risk prediction.\n",
        "\n",
        "Supports better financial decisions and risk management."
      ],
      "metadata": {
        "id": "YdtI43tbOVQE"
      }
    }
  ]
}